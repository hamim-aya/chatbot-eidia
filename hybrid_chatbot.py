"""
Chatbot Hybride LSTM + LLM pour l'EIDIA-UEMF
Utilise un mod√®le LSTM pour la classification d'intentions
et un LLM (Llama 3, GPT, etc.) pour reformuler les r√©ponses
"""

from chatbot import ChatbotModel
from llm_provider import LLMProvider, GeminiProvider
from typing import Dict, Any, Optional
import os
from dotenv import load_dotenv
load_dotenv()

class HybridChatbot:
    """
    Chatbot hybride combinant classification LSTM et g√©n√©ration LLM
    
    Architecture:
    1. Question utilisateur ‚Üí LSTM ‚Üí Tag d'intention + Confiance
    2. Tag ‚Üí JSON responses ‚Üí R√©ponse brute
    3. R√©ponse brute ‚Üí LLM ‚Üí R√©ponse reformul√©e (naturelle)
    """
    
    # System Prompt optimis√© pour l'EIDIA-UEMF
    SYSTEM_PROMPT = """Tu es l'assistant virtuel officiel de l'EIDIA (√âcole Euro-Med d'Ing√©nierie Digitale et d'Intelligence Artificielle), qui fait partie de l'UEMF √† F√®s, Maroc.

üéØ TON R√îLE:
Tu aides les √©tudiants et candidats en reformulant les informations officielles de mani√®re chaleureuse, claire et engageante, SANS JAMAIS inventer de nouvelles informations.

üìã R√àGLES STRICTES:
1. ‚úÖ REFORMULE la r√©ponse fournie pour la rendre plus humaine et conversationnelle
2. ‚úÖ CONSERVE TOUTES les informations factuelles (dates, prix, noms, modules, salaires)
3. ‚úÖ GARDE les √©mojis et la structure si elle aide √† la lisibilit√©
4. ‚ùå N'INVENTE JAMAIS de nouvelles informations (modules, profs, d√©bouch√©s, prix)
5. ‚ùå NE SUPPRIME AUCUNE information importante de la r√©ponse originale
6. ‚úÖ Adapte le ton selon le contexte (professionnel pour admission, plus l√©ger pour vie √©tudiante)
7. ‚úÖ Si la r√©ponse contient des listes, garde-les claires et structur√©es

üéì CONTEXTE EIDIA:
L'EIDIA forme des ing√©nieurs d'√âtat en 5 ans (2 ans pr√©pa + 3 ans ing√©nieur) dans 5 fili√®res:
- Big Data & Analytique (Pr. Loubna Ourabah)
- Intelligence Artificielle (Pr. Asmae Abadi)
- Robotique & Cobotique (Pr. Bader El Kari)
- Cybers√©curit√© & Computer Science (Pr. Taha)
- Full Stack Engineering & Multim√©dia (Pr. Mouhtadi Meryem)

üí¨ STYLE:
- Ton chaleureux mais professionnel
- Phrases courtes et claires
- Encourage l'√©tudiant dans son projet
- Termine par une ouverture si appropri√©

Reformule maintenant la r√©ponse en respectant ces r√®gles!"""

    def __init__(
        self, 
        lstm_model: Optional[ChatbotModel] = None,
        llm_provider: Optional[LLMProvider] = None,
        use_llm: bool = True
    ):
        """
        Initialise le chatbot hybride
        
        Args:
            lstm_model: Instance de ChatbotModel (si None, en cr√©e une nouvelle)
            llm_provider: Provider LLM (Ollama, OpenAI, etc.)
            use_llm: Si False, utilise seulement le LSTM sans reformulation
        """
        # Initialiser le mod√®le LSTM
        self.lstm_model = lstm_model or ChatbotModel()
        print("üìä Chargement du mod√®le LSTM...")
        if not self.lstm_model.load_model():
            raise RuntimeError("Impossible de charger le mod√®le LSTM")
        self.lstm_model.load_responses()
        
        # Initialiser le provider LLM
        self.use_llm = use_llm
        if use_llm:
            self.llm_provider = llm_provider or self._auto_detect_provider()
        else:
            print("‚ö†Ô∏è  Mode LSTM seul (pas de reformulation LLM)")
            self.llm_provider = None
    
    def _auto_detect_provider(self) -> LLMProvider:
        """D√©tecte automatiquement le provider Gemini"""
        gemini_key = os.getenv("GEMINI_API_KEY")
        if gemini_key:
            print("üîç Gemini d√©tect√©, utilisation de Gemini Flash...")
            return GeminiProvider(api_key=gemini_key)
        else:
            raise ValueError(
                "‚ùå Cl√© API Gemini non trouv√©e!\n"
                "D√©finissez la variable d'environnement GEMINI_API_KEY"
            )
    
    def chat(
        self, 
        user_question: str, 
        reformulate: bool = True,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Dict[str, Any]:
        """
        Pipeline complet du chatbot hybride
        
        Args:
            user_question: Question de l'utilisateur
            reformulate: Si True, reformule avec le LLM
            temperature: Temp√©rature du LLM (0.0 = d√©terministe, 1.0 = cr√©atif)
            max_tokens: Nombre maximum de tokens g√©n√©r√©s
        
        Returns:
            Dict contenant:
                - question: Question originale
                - intent: Tag d'intention d√©tect√©
                - confidence: Confiance de la pr√©diction (0-1)
                - raw_response: R√©ponse brute du JSON
                - final_response: R√©ponse reformul√©e (ou brute si pas de LLM)
                - reformulated: True si reformul√©e par LLM
        """
        print(f"\n{'='*70}")
        print(f"üí¨ Question: {user_question}")
        print(f"{'='*70}")
        
        # √âtape 1: Classification LSTM
        print("\n1Ô∏è‚É£ Classification LSTM...")
        lstm_result = self.lstm_model.chat_response(user_question)
        
        intent = lstm_result['intent']
        confidence = lstm_result['confidence']
        raw_response = lstm_result['response']
        
        print(f"   üéØ Intention: {intent}")
        print(f"   üìä Confiance: {confidence:.1%}")
        
        # √âtape 2: Reformulation LLM (si activ√©e et disponible)
        if reformulate and self.use_llm and self.llm_provider:
            print("\n2Ô∏è‚É£ Reformulation avec LLM...")
            try:
                final_response = self._reformulate_with_llm(
                    user_question=user_question,
                    intent=intent,
                    raw_response=raw_response,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                reformulated = True
                print("   ‚úÖ R√©ponse reformul√©e avec succ√®s")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur LLM: {e}")
                print("   ‚Ü©Ô∏è  Retour √† la r√©ponse brute")
                final_response = raw_response
                reformulated = False
        else:
            final_response = raw_response
            reformulated = False
            print("\n2Ô∏è‚É£ Pas de reformulation (mode LSTM seul)")
        
        return {
            'question': user_question,
            'intent': intent,
            'confidence': confidence,
            'raw_response': raw_response,
            'final_response': final_response,
            'reformulated': reformulated
        }
    
    def _reformulate_with_llm(
        self,
        user_question: str,
        intent: str,
        raw_response: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> str:
        """
        Reformule la r√©ponse brute avec le LLM
        
        Args:
            user_question: Question originale de l'utilisateur
            intent: Tag d'intention d√©tect√©
            raw_response: R√©ponse brute du JSON
            temperature: Cr√©ativit√© du LLM
            max_tokens: Longueur maximale
        
        Returns:
            R√©ponse reformul√©e par le LLM
        """
        # Construire le message utilisateur pour le LLM
        user_message = f"""QUESTION DE L'√âTUDIANT:
"{user_question}"

INTENTION D√âTECT√âE: {intent}

R√âPONSE BRUTE √Ä REFORMULER:
{raw_response}

Reformule cette r√©ponse de mani√®re chaleureuse et naturelle tout en conservant TOUTES les informations factuelles."""

        # Appeler le LLM
        reformulated = self.llm_provider.generate(
            system_prompt=self.SYSTEM_PROMPT,
            user_message=user_message,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        return reformulated.strip()
    
    def interactive(self, reformulate: bool = True):
        """
        Mode interactif du chatbot hybride
        
        Args:
            reformulate: Si True, utilise la reformulation LLM
        """
        mode = "HYBRIDE (LSTM + LLM)" if reformulate else "LSTM SEUL"
        print(f"\n{'='*70}")
        print(f"ü§ñ CHATBOT EIDIA-UEMF - MODE {mode}")
        print(f"{'='*70}")
        print("Tapez 'quit' pour quitter")
        print("Tapez 'toggle' pour changer de mode")
        print()
        
        while True:
            try:
                user_input = input("Vous: ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("\nüëã Au revoir ! N'h√©sitez pas √† revenir pour toute question sur l'EIDIA.")
                    break
                
                if user_input.lower() == 'toggle':
                    reformulate = not reformulate
                    mode = "HYBRIDE (LSTM + LLM)" if reformulate else "LSTM SEUL"
                    print(f"\nüîÑ Passage en mode {mode}\n")
                    continue
                
                # Obtenir la r√©ponse
                result = self.chat(user_input, reformulate=reformulate)
                
                # Afficher la r√©ponse
                print(f"\nü§ñ Bot [{result['intent']} - {result['confidence']:.0%}]:")
                print(result['final_response'])
                
                # Afficher si reformul√©
                if result['reformulated']:
                    print("\nüí° (R√©ponse reformul√©e par LLM)")
                
                print()
                
            except KeyboardInterrupt:
                print("\n\nüëã Au revoir !")
                break
            except Exception as e:
                print(f"\n‚ùå Erreur: {e}\n")


# Fonction utilitaire pour cr√©er rapidement un chatbot
def create_hybrid_chatbot(
    model: str = None,
    api_key: str = None,
    use_llm: bool = True
) -> HybridChatbot:
    """
    Cr√©e un chatbot hybride avec Gemini
    
    Args:
        model: Nom du mod√®le Gemini (gemini-3-flash-preview, gemini-1.5-pro, etc.)
        api_key: Cl√© API Gemini
        use_llm: Si False, utilise seulement le LSTM
    
    Returns:
        Instance de HybridChatbot
    """
    if not use_llm:
        return HybridChatbot(use_llm=False)
    
    llm = GeminiProvider(
        model=model or "gemini-3-flash-preview",
        api_key=api_key
    )
    
    return HybridChatbot(llm_provider=llm)


if __name__ == "__main__":
    # Exemple d'utilisation
    print("üöÄ Initialisation du chatbot hybride EIDIA-UEMF...")
    
    # Cr√©er le chatbot avec Gemini (utilise GEMINI_API_KEY du .env)
    chatbot = create_hybrid_chatbot()
    
    # Mode interactif
    chatbot.interactive(reformulate=True)
